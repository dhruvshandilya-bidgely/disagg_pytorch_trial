"""
Author - Prasoon Patidar
Date - 08th June 2020
Small utility functions for lifestyle module
"""

# import python packages

import copy
import logging
import numpy as np
import pandas as pd
from datetime import datetime
from scipy.spatial.distance import cdist

# import functions from within the project

from python3.config.Cgbdisagg import Cgbdisagg
from python3.utils.logs_utils import log_prefix
from python3.utils.time.get_time_diff import get_time_diff
from python3.config.mappings.get_app_id import get_app_id


def extract_appliance_attribute(appliance_profile, billcycle_start, logger, appliance=None, attribute=None):

    """
    Parameters:
        appliance_profile (dict)                   : Dictionary containing head of appliance profile generated by appliances
        billcycle_start(int)                       : Epoch timestamp for billcycle start
        logger(logging.Logger)                     : logger for utility function
        appliance      (dict)                      : Appliance for which we need to find value
        attribute (dict)                           : Attribute of appliance we are trying to find

    Returns:
        attribute_value(object)                    : attribute value from appliance profile
    """

    # Get appliance attributes for represented billcycle_start

    START_IDX = 0
    appliance_profile_bc = appliance_profile.get(int(billcycle_start))

    appliance_profile_bc = appliance_profile_bc.get('profileList')[START_IDX]

    appliance_present = True

    if appliance is None:
        logger.info("%s Got appliance profile for bill cycle %d ", log_prefix('Generic'), billcycle_start)

        attribute_value = appliance_profile_bc

        appliance_present = False

    if appliance_present:

        # Get AppId for given appliance(string format)

        app_id = str(get_app_id(appliance))

        # check if profile exists for this appliance Id

        appliance_specific_profile = appliance_profile_bc.get(app_id)

        if appliance_specific_profile is None:
            logger.error("%s Unable to get appliance %s (id: %s) in appliance profile", log_prefix('Generic'),
                         appliance, app_id)

            return None

        # get all attributes for this app id

        appliance_attributes = appliance_specific_profile[START_IDX].get('attributes')

        # check if a particular attribute is provided in input

        if attribute is None:
            logger.info("%s Got appliance attributes for bill cycle %d, appliance %s ", log_prefix('Generic'),
                        billcycle_start, str(appliance))

            return appliance_attributes

        # Check if given attribute is provided

        appliance_specific_attribute = appliance_attributes.get(attribute)

        if appliance_specific_attribute is None:

            logger.error("%s Unable to get appliance %s (id: %s) attribute %s in appliance profile",
                         log_prefix('Generic'), str(appliance), str(app_id), str(attribute))
        else:

            logger.info("%s Got appliance %s (id: %s) attribute %s (value: %s) in appliance profile",
                        log_prefix('Generic'), str(appliance), str(app_id), str(attribute), str(appliance_specific_attribute))

        attribute_value = appliance_specific_attribute

    return attribute_value


def get_consumption_level(input_data, pilot_config, consumption_levels, lifestyle_hsm, logger_pass, annual_tag):

    """
    Parameters:
        input_data (np.ndarray)                    : Custom trimmed input data
        pilot_config(dict)                         : pilot dependent static config
        consumption_levels(enum.Enum)              : Class to define various consumption levels
        logger_pass(dict)                          : Contains base logger and logging dictionary
    Returns:
        consumption_level(enum.Enum)               : consumption level for given input data
    """

    t_consumption_level_start = datetime.now()

    # Initialize the logger

    logger_pass = dict(logger_pass)
    logger_base = logger_pass.get('logger_base').getChild('get_consumption_level')
    logger = logging.LoggerAdapter(logger_base, logger_pass.get('logging_dict'))
    logger_pass['logger_base'] = logger_base

    # get No. of unique days in input data

    unique_day_vals = np.unique(input_data[:, Cgbdisagg.INPUT_DAY_IDX])

    num_unique_days = unique_day_vals.shape[0]

    DAYS_IN_YEAR = Cgbdisagg.DAYS_IN_YEAR

    consumption_level_buckets = pilot_config.get('annual_consumption_level')

    user_consumption_bucket = -1

    # return empty array if input data is empty

    if input_data.shape[0] == 0:
        logger.info("%s No consumption values in input data..",
                    log_prefix(['SeasonalLoadType', 'DailyLoadType'], type='list'))

        user_consumption_bucket = 0

        consumption_value = 0

    # get total consumption and scale it at annual level in input data

    total_consumption = np.sum(input_data[:, Cgbdisagg.INPUT_CONSUMPTION_IDX])

    if total_consumption < 0:
        # Handle negative consumption

        logger.error("%s Negative consumption values in input data..",
                     log_prefix(['SeasonalLoadType', 'DailyLoadType'], type='list'))

        user_consumption_bucket = 0

        consumption_value = 0

    if user_consumption_bucket == 0:
        return consumption_levels(user_consumption_bucket).name, consumption_level_buckets, consumption_value

    scaled_consumption_kwh = (total_consumption * DAYS_IN_YEAR) / (num_unique_days * 1e3)

    # get boundaries for annual consumption level

    annual_consumption_boundaries = np.array(pilot_config.get('annual_consumption_level'))

    # Get the bucket this scaled consumption belong to based on annual boundaries

    higher_consumption_buckets = np.where(annual_consumption_boundaries > scaled_consumption_kwh)[0]

    if higher_consumption_buckets.shape[0] == 0:

        # Handle case when scaled consumption if higher than upper limit

        user_consumption_bucket = consumption_levels.count.value - 1

    else:

        user_consumption_bucket = higher_consumption_buckets[0]

    user_consumption_level = consumption_levels(user_consumption_bucket).name

    cons_level_checks_based_on_prev_run_not_req = (not annual_tag) or ((lifestyle_hsm is None) or (lifestyle_hsm.get('attributes') is None))
    # Adjustment based on previous run output

    if cons_level_checks_based_on_prev_run_not_req:
        t_consumption_level_end = datetime.now()

        logger.debug("%s Getting consumption levels took | % .3f s",
                     log_prefix(['SeasonalLoadType', 'DailyLoadType'], type='list'),
                     get_time_diff(t_consumption_level_start, t_consumption_level_end))
        return user_consumption_level, consumption_level_buckets, scaled_consumption_kwh

    cons_level_soft_margin_frac_thres = 0.1

    # updating consumption level tag based on previous run output

    if lifestyle_hsm.get('attributes').get('consumption_value') is None:
        logger.warning('consumption level attribute is missing in lifestyle HSM | ')
    else:

        # fetching HSM info

        hsm_in = lifestyle_hsm.get('attributes')

        consumption_bucket_list = np.array(['not_known', 'low', 'low_mid', 'mid', 'mid_high', 'high'])

        hsm_consumption_level = consumption_bucket_list[int(hsm_in.get('consumption_level')[0])]
        hsm_consumption_level_buckets = hsm_in.get('consumption_level_buckets')

        if user_consumption_level != hsm_consumption_level:

            consumption_bucket_soft_margin = [hsm_consumption_level_buckets[int(hsm_in.get('consumption_level')[0])-1],
                                              hsm_consumption_level_buckets[int(hsm_in.get('consumption_level')[0])]]

            # checking if annual cons is close to previous run total consumption
            # if yes, the attribute tag is kept similar to previous run

            bucket_size = consumption_bucket_soft_margin[1] - consumption_bucket_soft_margin[0]

            soft_margin_thres = cons_level_soft_margin_frac_thres * bucket_size

            consumption_bucket_soft_margin[0] = consumption_bucket_soft_margin[0] - soft_margin_thres
            consumption_bucket_soft_margin[1] = consumption_bucket_soft_margin[1] + soft_margin_thres

            if scaled_consumption_kwh < consumption_bucket_soft_margin[1] and \
                    scaled_consumption_kwh > consumption_bucket_soft_margin[0]:

                logger.info('Original calculated consumption level | %s ', user_consumption_level)

                user_consumption_level = hsm_consumption_level

                logger.info('Updating consumption level to | %s ', hsm_consumption_level)

    t_consumption_level_end = datetime.now()

    logger.debug("%s Getting consumption levels took | % .3f s",
                 log_prefix(['SeasonalLoadType', 'DailyLoadType'],type = 'list'),
                 get_time_diff(t_consumption_level_start, t_consumption_level_end))

    return user_consumption_level, consumption_level_buckets, scaled_consumption_kwh


def get_yearly_load_type(lifestyle_input_object, day_input_data, day_input_idx, kmeans_yearly_model, yearly_load_types, lifestyle_hsm, logger_pass):

    """
    Parameters:
        lifestyle_input_object (dict)              : Dictionary containing all inputs for lifestyle modules
        day_input_data (np.ndarray)                : Custom trimmed input data in 2-D format
        day_input_idx  (np.ndarray)                : day index for custom trimmed input
        kmeans_yearly_model(dict)                  : kmeans yearly model stored for user
        yearly_load_types(enum.Enum)               : yearly load types Enum
        logger_pass(dict)                          : Contains base logger and logging dictionary
    Returns:
        yearly_load_type(string)                   : Seasonal load pattern for this user
    """

    t_yearly_load_type_start = datetime.now()

    # Initialize the logger

    logger_pass = dict(logger_pass)
    logger_base = logger_pass.get('logger_base').getChild('get_yearly_load_type')
    logger = logging.LoggerAdapter(logger_base, logger_pass.get('logging_dict'))
    logger_pass['logger_base'] = logger_base

    # truncate day_input_data and day_input_idx to last 365 days(removing extra 1-2 days)
    # TODO(Nisha): need to look why is it happening

    DAY_IN_YEAR = Cgbdisagg.DAYS_IN_YEAR

    day_input_idx = day_input_idx[-DAY_IN_YEAR:]

    day_input_data = day_input_data[-DAY_IN_YEAR:]

    # Get month-day in string format from day level idx

    day_monthday_vals = pd.to_datetime(day_input_idx, unit='s').strftime('%m-%d').values

    # Get total daily consumption for each day

    day_total_consumption = np.nansum(day_input_data, axis=1)

    day_total_consumption[np.isnan(day_total_consumption)] = 0

    # TODO(Nisha): Added this to handle daylight savings issue causing break in code, need to be fixed from BE

    monthday_vals_unique, monthday_vals_rev_idx, monthday_vals_counts = np.unique(day_monthday_vals,
                                                                                  return_inverse=True,
                                                                                  return_counts=True)

    day_total_consumption = np.bincount(monthday_vals_rev_idx, weights=day_total_consumption)

    day_monthday_vals = monthday_vals_unique

    # Get normed daily consumption for each day

    min_consumption_val = np.percentile(day_total_consumption[day_total_consumption > 0], 3)

    max_consumption_val = np.percentile(day_total_consumption[day_total_consumption > 0], 97)

    day_normed_consumption = day_total_consumption - min_consumption_val

    day_normed_consumption /= max_consumption_val - min_consumption_val

    day_normed_consumption[day_normed_consumption < 0] = 0.

    day_normed_consumption = np.fmin(1, day_normed_consumption)

    # Get month-dates-map from kmeans yearly model and get common month_dates_map

    month_dates_map = np.array(kmeans_yearly_model.get('month_dates_map'))

    # get available indexes in month_dates_map and reorient cluster centers accordingly

    cluster_centers = kmeans_yearly_model.get('cluster_centers')

    month_dates_map_idx = np.isin(month_dates_map, day_monthday_vals)

    cluster_centers = cluster_centers[:, month_dates_map_idx]

    # redo the month-dates indexing to remove any days in step 2

    month_dates_map = month_dates_map[month_dates_map_idx]

    day_monthday_vals_idx = np.isin(day_monthday_vals, month_dates_map)

    day_monthday_vals = day_monthday_vals[day_monthday_vals_idx]

    day_normed_consumption = day_normed_consumption[day_monthday_vals_idx]

    # sort day month val index and corresponding data

    sorted_daymonth_val_idx = np.argsort(day_monthday_vals)

    day_normed_consumption = day_normed_consumption[sorted_daymonth_val_idx]

    # get distance of day normed consumption from each cluster center

    cluster_distances = cdist(day_normed_consumption.reshape(1, -1), cluster_centers, 'euclidean')[0]

    cluster_id = np.argmin(cluster_distances)

    yearly_load_soft_margin_frac_thres = 0.1

    # updating yearly load type based on previous run output

    yearly_load_type = yearly_load_types[cluster_id]

    # print(cluster_distances[int(cluster_id)])

    if (lifestyle_hsm is None) or (lifestyle_hsm.get('attributes') is None):
        logger.warning('Lifestyle HSM attributes are absent | ')

    else:
        if lifestyle_hsm.get('attributes').get('yearly_load_type') is None:
            logger.warning('yearly load type attribute is missing in lifestyle HSM | ')
        else:
            # fetching HSM info

            hsm_in = lifestyle_hsm.get('attributes')

            cluster_labels = np.array(lifestyle_input_object.get('yearly_load_type_list'))
            hsm_yearly_load_type = cluster_labels[int(hsm_in.get('yearly_load_type')[0])]
            kmeans_yearly_model['cluster_labels'] = np.array(kmeans_yearly_model.get('cluster_labels'))

            # checking if current run least euclidean dist is close to previous one
            # if yes, the attribute tag is kept similar to previous run

            if int(hsm_in.get('yearly_load_type')[0]) != -1 and \
                    np.any(kmeans_yearly_model.get('cluster_labels') == hsm_yearly_load_type) and \
                    cluster_id != np.where(kmeans_yearly_model.get('cluster_labels') == hsm_yearly_load_type)[0][0]:

                hsm_yearly_load_type_id = np.where(kmeans_yearly_model.get('cluster_labels') == hsm_yearly_load_type)[0][0]
                hsm_yearly_load_type_distance = hsm_in.get('yearly_load_type_distance')[0]

                yearly_load_type_soft_margin = [hsm_yearly_load_type_distance * (1 - yearly_load_soft_margin_frac_thres),
                                                hsm_yearly_load_type_distance * (1 + yearly_load_soft_margin_frac_thres)]

                cluster_dist_of_prev_load_type = cluster_distances[int(hsm_yearly_load_type_id)]

                if cluster_dist_of_prev_load_type < yearly_load_type_soft_margin[1] and \
                        cluster_dist_of_prev_load_type > yearly_load_type_soft_margin[0]:

                    logger.info('Original calculated yearly load type | %s ', kmeans_yearly_model['cluster_labels'][int(cluster_id)])

                    cluster_id = hsm_yearly_load_type_id
                    yearly_load_type = yearly_load_types[int(cluster_id)]

                    logger.info('Updating yearly load type to | %s ', kmeans_yearly_model['cluster_labels'][int(cluster_id)])

    yearly_load_debug = {
        'cluster_distances': cluster_distances,
        'min_cons_val'     : min_consumption_val,
        'max_cons_val'     : max_consumption_val,
        'year_data_normed' : day_normed_consumption,
        'year_data_idx'    : day_monthday_vals[sorted_daymonth_val_idx],
        'cluster_id'       : cluster_id,
        'yearly_load_type' : yearly_load_type,
    }

    t_yearly_load_type_end = datetime.now()

    logger.info("%s Getting yearly load types took | %.3f s", log_prefix('SeasonalLoadType'),
                get_time_diff(t_yearly_load_type_start, t_yearly_load_type_end))

    return yearly_load_type, yearly_load_debug


def get_hour_fraction(input_data, day_input_data, day_input_idx):
    """
    Parameters:
        input_data (np.ndarray)                    : Custom trimmed input data in 15 col matrix
        day_input_data (np.ndarray)                : all input data in 2-D format
        day_input_idx  (np.ndarray)                : day index for all input data
    Returns:
        hour_fraction(np.ndarray)                  : fraction of consumption in each hour of given data
    """

    input_days_unique = np.unique(input_data[:, Cgbdisagg.INPUT_DAY_IDX])

    day_input_idx_valid = np.isin(day_input_idx, input_days_unique)

    day_input_data_valid = day_input_data[day_input_idx_valid]

    if day_input_data_valid.shape[0] == 0:
        hour_fraction = np.zeros(input_data.shape[1])

        return hour_fraction

    hourly_consumption = np.nansum(day_input_data_valid, axis=0)

    hour_fraction = hourly_consumption / np.sum(hourly_consumption)

    hour_fraction = hour_fraction.round(2)

    return hour_fraction


def get_vacation_percentage(lifestyle_input_object, disagg_input_object, disagg_output_object):

    """
    Parameters:
        lifestyle_input_object(dict)            : Dictionary containing all inputs for lifestyle modules and submodules
        disagg_input_object(dict)               : Dictionary containing all disagg inputs
        disagg_output_object(dict)              : Dictionary containing all disagg outputs

    Returns:
        vacation_percentage(np.ndarray)         : vacation percentage for each bill cycle
    """

    # Retrieve lifestyle input data and module seq

    raw_input_data = lifestyle_input_object.get('original_input_data')

    # get raw vacation estimate from disagg output object

    module_seq = disagg_input_object.get('config').get('module_seq')

    # Initialize raw vacation epoch estimate

    vacation_epoch_estimate = np.zeros(raw_input_data.shape[0])

    if 'va' in module_seq:

        # If hvac in modules run, then get cooling information from here

        V1_WRITE_IDX, V2_WRITE_IDX = disagg_output_object.get('output_write_idx_map').get('va')

        disagg_epoch_estimate = disagg_output_object.get('epoch_estimate', None)

        if disagg_epoch_estimate is not None:
            # Get cooling and heating epoch level estimates

            vacation_epoch_estimate = disagg_epoch_estimate[:, V1_WRITE_IDX] + disagg_epoch_estimate[:, V2_WRITE_IDX]

            # remove nan values

            vacation_epoch_estimate[np.isnan(vacation_epoch_estimate)] = 0.

    # get min and max days for lifestyle input data and trim cooling and heating data

    lf_day_idx = lifestyle_input_object.get('day_input_data_index')

    min_day_idx = np.min(lf_day_idx)

    max_day_idx = np.max(lf_day_idx)

    # get bool value for vacation

    vacation_epoch_estimate = vacation_epoch_estimate > 0

    # trim vacation estimate

    vacation_estimate_valid = (raw_input_data[:, Cgbdisagg.INPUT_DAY_IDX] >= min_day_idx) & \
                              (raw_input_data[:, Cgbdisagg.INPUT_DAY_IDX] <= max_day_idx)

    # Get valid billcycle and vacation

    vacation_estimate = vacation_epoch_estimate[vacation_estimate_valid]

    input_billcycle = raw_input_data[vacation_estimate_valid, Cgbdisagg.INPUT_BILL_CYCLE_IDX]

    # get billcycle unique, inverse and counts

    billcycle_vals_unique, billcycle_vals_inv_idx, billcycle_vals_counts = np.unique(input_billcycle,
                                                                                     return_inverse=True,
                                                                                     return_counts=True)

    vacation_estimate_sum = np.bincount(billcycle_vals_inv_idx, weights=vacation_estimate)

    vacation_percentage = vacation_estimate_sum * 100 / billcycle_vals_counts

    # get day level vacation values

    input_days = raw_input_data[vacation_estimate_valid, Cgbdisagg.INPUT_DAY_IDX]

    # get days unique, inverse and counts

    days_vals_unique, days_vals_inv_idx, days_vals_counts = np.unique(input_days, return_inverse=True,
                                                                      return_counts=True)

    is_vacation_day = np.bincount(days_vals_inv_idx, weights=vacation_estimate)

    is_vacation_day[is_vacation_day > 0] = 1

    return np.array([billcycle_vals_unique, vacation_percentage]).T, np.array([days_vals_unique, is_vacation_day]).T


def get_day_level_2d_matrix(hourly_input_data, logger_pass):

    """
    Parameters:
        hourly_input_data(np.ndarray)            : Raw input for lifestyle modules at hourly level
        logger_pass(dict)                        : Contains base logger and logging dictionary

    Returns:
        hourly_input_data(np.ndarray)            : 2-D Array for all processed lifestyle inputs
        new_hourly_input_data_idx(dict)          : Dictionary containing index information for lifestyle input data
    """

    t_day_level_2d_matrix_start = datetime.now()

    # Initialize the logger

    logger_pass = dict(logger_pass)
    logger_base = logger_pass.get('logger_base').getChild('get_day_level_2d_matrix')
    logger = logging.LoggerAdapter(logger_base, logger_pass.get('logging_dict'))
    logger_pass['logger_base'] = logger_base

    # get unique day_vals and corresponding row_idx

    day_vals, day_row_idx = np.unique(hourly_input_data[:, Cgbdisagg.INPUT_DAY_IDX], return_inverse=True)

    # Initialize 2d matrix with default value of nan

    day_data = np.full(shape=(day_vals.shape[0], Cgbdisagg.HRS_IN_DAY), fill_value=np.nan)

    logger.debug("Shape of converted 2d day wise data is | %s", str(day_data.shape))

    # Compute hour of day based indices to use

    col_idx = hourly_input_data[:, Cgbdisagg.INPUT_HOD_IDX].astype(int)

    # Create day wise 2d arrays for each input data row accordingly

    day_data[day_row_idx, col_idx] = hourly_input_data[:, Cgbdisagg.INPUT_CONSUMPTION_IDX]

    t_day_level_2d_matrix_end = datetime.now()

    logger.info('Getting 2D matrix for day level data took | %.3f s',
                get_time_diff(t_day_level_2d_matrix_start,
                              t_day_level_2d_matrix_end))

    return day_vals, day_data


def get_weekend_warrior_input(disagg_input_object, lifestyle_input_object, logger_pass):
    """
    Parameters:
        disagg_input_object (np.ndarray)           : Dictionary containing all inputs for disagg modules and submodules
        lifestyle_input_object(dict)               : Dictionary containing all inputs for lifestyle modules and submodules
        logger_pass(dict)                          : Contains base logger and logging dictionary
    Returns:
        weekend_warrior_input_object(string)       : get relevant input for weekend warrior calculation
    """

    t_weekend_warrior_input_start = datetime.now()

    # Initialize the logger

    logger_pass = dict(logger_pass)
    logger_base = logger_pass.get('logger_base').getChild('get_weekend_warrior_input')
    logger = logging.LoggerAdapter(logger_base, logger_pass.get('logging_dict'))
    logger_pass['logger_base'] = logger_base
    logger.debug("%s Start: process sampling rate input for weekend warrior calculation", log_prefix('WeekendWarrior'))

    # Initialize weekend_warrior_input_object and get relevant configs

    weekend_warrior_input_object = dict()

    raw_data_config = lifestyle_input_object.get('raw_data_config')

    sampling_rate = lifestyle_input_object.get('sampling_rate')

    # Get sample rate input data from disagg input object

    input_data = copy.deepcopy(lifestyle_input_object.get('original_input_data'))

    # Trim number of days to max allowed days

    max_day_val = np.max(input_data[:, Cgbdisagg.INPUT_DAY_IDX])
    min_day_val = np.min(input_data[:, Cgbdisagg.INPUT_DAY_IDX])

    max_allowed_epoch_difference = Cgbdisagg.SEC_IN_DAY * raw_data_config.get('num_days_limit')

    if (max_day_val - min_day_val) >= max_allowed_epoch_difference:
        min_allowed_day_val = max_day_val - max_allowed_epoch_difference
        input_data = input_data[input_data[:, Cgbdisagg.INPUT_DAY_IDX] >= min_allowed_day_val]

    # Trim consumption values to remove outlier consumption

    max_percentile_val = raw_data_config.get('max_percentile_val')
    max_allowed_consumption_val = np.percentile(input_data[:, Cgbdisagg.INPUT_CONSUMPTION_IDX], max_percentile_val)

    consumption_fill_value = np.percentile(input_data[:, Cgbdisagg.INPUT_CONSUMPTION_IDX], max_percentile_val + 1)

    input_data[
        input_data[:, Cgbdisagg.INPUT_CONSUMPTION_IDX] > max_allowed_consumption_val, Cgbdisagg.INPUT_CONSUMPTION_IDX] = \
        consumption_fill_value

    # get unique day_vals and corresponding row_idx

    day_vals, day_row_idx = np.unique(input_data[:, Cgbdisagg.INPUT_DAY_IDX], return_inverse=True)

    # Compute column indices to use

    SAMPLES_PER_DAY = int(Cgbdisagg.SEC_IN_DAY / sampling_rate)

    SAMPLES_PER_HOUR = int(Cgbdisagg.SEC_IN_HOUR / sampling_rate)

    # Initialize 2d matrix with default value of nan

    day_data = np.full(shape=(day_vals.shape[0], SAMPLES_PER_DAY), fill_value=np.nan)

    logger.debug("%s Shape of converted 2d day wise data for weekend warrior is | %s", log_prefix('WeekendWarrior'),
                 str(day_data.shape))

    # get col_idx based on sampling rate

    col_idx = input_data[:, Cgbdisagg.INPUT_EPOCH_IDX] - input_data[:, Cgbdisagg.INPUT_DAY_IDX]
    col_idx = col_idx / Cgbdisagg.SEC_IN_HOUR
    col_idx = (SAMPLES_PER_HOUR * (col_idx - col_idx.astype(int) + input_data[:, Cgbdisagg.INPUT_HOD_IDX])).astype(int)

    # get day level data for sampling rate

    day_data[day_row_idx, col_idx] = input_data[:, Cgbdisagg.INPUT_CONSUMPTION_IDX]

    # write day data and indices in weekend warrior input object

    weekend_warrior_input_object.update({
        'sample_rate_day_data': day_data,
        'sample_rate_day_idx' : day_vals
    })

    t_weekend_warrior_input_end = datetime.now()

    logger.debug("%s Got Weekend Warrior input in | %.3f s", log_prefix('WeekendWarrior'),
                 get_time_diff(t_weekend_warrior_input_start, t_weekend_warrior_input_end))

    return weekend_warrior_input_object


def populate_lifestyle_hsm(lifestyle_output_object, analytics_input_object, analytics_output_object):

    """
    update lifestyle HSM

    Parameters:
        lifestyle_output_object             (dict)         : Dictionary containing all outputs for lifestyle modules and submodules
        analytics_input_object              (dict)         : Dictionary containing information about analytics input
        analytics_output_object             (dict)         : Dictionary containing information about analytics output

    Returns:
        analytics_output_object             (dict)         : Dictionary containing information about analytics output
    """

    yearly_load_type_min_dist =  lifestyle_output_object.get('annual').get('yearly_load_debug').get('cluster_distances')[
        int(lifestyle_output_object.get('annual').get('yearly_load_debug').get('cluster_id'))]

    consumption_bucket_list = np.array(['not_known', 'low', 'low_mid', 'mid', 'mid_high', 'high'])

    consumption_level_tag_index = np.where(consumption_bucket_list == lifestyle_output_object.get('annual').get('consumption_level'))[0][0]

    results = {
        'consumption_value': lifestyle_output_object.get('annual').get('consumption_value'),
        'consumption_level_buckets': lifestyle_output_object.get('annual').get('consumption_level_buckets'),
        'consumption_level': consumption_level_tag_index,
        'yearly_load_type': lifestyle_output_object.get('annual').get('yearly_load_debug').get('yearly_load_cluster_id'),
        'yearly_load_type_distance': yearly_load_type_min_dist,
        'office_goer_score': lifestyle_output_object.get('annual').get('office_goer_prob'),
        'active_user_score': lifestyle_output_object.get('annual').get('active_user_prob'),
        'weekend_warrior_score': lifestyle_output_object.get('annual').get('weekend_warrior_prob')
    }

    hsm_output = {
        'timestamp': analytics_input_object['input_data'][:, Cgbdisagg.INPUT_EPOCH_IDX][-1],
        'attributes': results,
    }

    analytics_output_object['created_hsm']['li'] = hsm_output

    return analytics_output_object
